# Skeldir Backend Cursor Rules
# Version: 1.0
# Purpose: LLM-assisted development guidance for acquisition-ready Skeldir Attribution Intelligence backend

---
general: |
  # Strategic Context: Private Equity Acquisition Readiness
  
  This codebase implements Skeldir Attribution Intelligence, a Bayesian statistical modeling platform for multi-touch attribution and revenue verification. Every architectural decision and code pattern must be explicitly optimized for Private Equity (PE) acquisition readiness, focusing on maintainability, modularity, rapid integration, and technical debt minimization.
  
  ## Core Architectural Mandates
  
  ### 1. Modular Monolith Architecture
  The backend is structured as a modular monolith with strict component boundaries:
  - `app/ingestion/`: Event ingestion service with idempotency and dead letter queue
  - `app/attribution/`: Statistical attribution models (Bayesian MMM, deterministic)
  - `app/auth/`: Authentication and authorization with tenant isolation
  - `app/webhooks/`: Webhook handlers for Shopify, WooCommerce, Stripe, PayPal
  
  Each component must be developed with decoupled boundaries and clear API contracts, as if they were separate systems. This enables future microservices extraction without architectural rewrites.
  
  ### 2. PostgreSQL-Only Stack (Unified Data System)
  **Strategic Rationale:** PE firms flag "disparate data systems" (Postgres + Redis + Kafka) as deal-killers because they prevent unified analytics layers and increase integration costs.
  
  **Mandatory Technology Choices:**
  - Database: PostgreSQL 15+ (exclusive data store)
  - Celery Broker: PostgreSQL (`db+postgresql://`), NOT Redis or RabbitMQ
  - Caching: HTTP ETag headers (30-60s TTL), NOT Redis
  - Event Processing: PostgreSQL tables with background workers, NOT Kafka
  
  **Prohibited Technologies:**
  - ❌ Redis / Memcached (use ETag caching and materialized views)
  - ❌ Kafka / Event Streaming (use PostgreSQL event tables)
  - ❌ NoSQL Databases (PostgreSQL JSONB handles semi-structured data)
  
  ### 3. Privacy-First Architecture by Design
  **Mandate:** Eliminate all identity resolution, cross-device tracking, and PII storage. This is an architectural constraint, not a feature toggle.
  
  **Required Patterns:**
  - Session-scoped events: `session_id` is ephemeral (30-minute inactivity timeout), NOT cross-session
  - No user identity resolution or probabilistic matching
  - All webhook data must pass through PII stripping before persistence
  - No IP address logging beyond rate limiting (memory-only)
  - Tenant data isolation via PostgreSQL Row-Level Security (RLS)
  
  ### 4. Contract-First API Development
  **Workflow:** OpenAPI 3.1 contract → Pydantic model generation → Implementation → Prism mock server → Frontend consumption
  
  **Requirements:**
  - All API contracts defined in `api-contracts/openapi/v1/` BEFORE implementation
  - Semantic versioning: `major.minor.patch` (major=breaking, minor=additive, patch=docs)
  - Breaking change detection via `oasdiff breaking` in CI/CD
  - Pydantic models auto-generated from contracts using `datamodel-codegen`
  
  ### 5. Scientific/Bayesian Modeling Core
  **Statistical Framework:**
  - PyMC-Marketing 0.3+ for Media Mix Modeling (MMM) and attribution
  - ArviZ 0.16+ for convergence diagnostics
  - scikit-learn 1.3+ for preprocessing pipelines
  
  **Model Validation Requirements:**
  - Bayesian models: R-hat < 1.01, ESS > 400, divergences == 0
  - Deterministic models: 100% allocation completeness (sum of allocations = conversion_value_cents)
  - All model outputs must include convergence diagnostics in responses
  
  ## Technology Stack
  
  - **API Framework:** FastAPI 0.104+ with Pydantic 2.0+
  - **Database:** PostgreSQL 15+ with Row-Level Security (RLS)
  - **Background Tasks:** Celery 5.3+ with PostgreSQL broker
  - **Observability:** OpenTelemetry, Prometheus, Grafana, Sentry
  - **Testing:** pytest, pytest-asyncio, contract tests (Dredd/Pact)

---
rules:
  - name: "Creating a new Attribution API endpoint"
    description: |
      When creating a new endpoint in `app/api/v1/attribution.py`, follow this template to enforce Skeldir's modular monolith, contract-first, and observability mandates.
      
      **Template Pattern:**
      1. Use FastAPI `APIRouter` with prefix `/api/attribution`
      2. Import Pydantic models from `app/schemas/attribution.py` (generated from OpenAPI contracts)
      3. Require `X-Correlation-ID` header (UUID format) for distributed tracing
      4. Inject `tenant_id` via `get_current_tenant()` dependency from `app/core/auth`
      5. Implement ETag caching (30s TTL) with `If-None-Match` header handling
      6. Convert revenue from cents (INTEGER) to dollars (float) in responses
      7. Compute `data_freshness_seconds` from `last_updated` timestamp
      
      **Reference Implementation:** `/api/attribution/revenue/realtime` endpoint in Architecture Guide B1.1
      
      **Example:**
      ```python
      from fastapi import APIRouter, Depends, Header, Response, status
      from sqlalchemy.ext.asyncio import AsyncSession
      from app.core.database import get_db
      from app.core.auth import get_current_tenant
      from app.schemas.attribution import YourResponseModel
      
      router = APIRouter(prefix="/api/attribution", tags=["Attribution"])
      
      @router.get("/your-endpoint", response_model=YourResponseModel)
      async def your_endpoint(
          response: Response,
          correlation_id: str = Header(..., alias="X-Correlation-ID"),
          tenant_id: str = Depends(get_current_tenant),
          db: AsyncSession = Depends(get_db),
      ) -> YourResponseModel:
          # Implementation with ETag caching, revenue conversion, data freshness
          pass
      ```
      
      **Required Patterns:**
      - All endpoints must include `X-Correlation-ID` header validation
      - All responses must include `tenant_id` (from dependency, not request body)
      - Revenue values stored as INTEGER (cents) in database, converted to float (dollars) in API responses
      - ETag caching with 30-second TTL for GET endpoints
      - Structured logging with `tenant_id`, `correlation_id`, `event_type` fields

  - name: "Creating a new Ingestion endpoint"
    description: |
      When creating event ingestion endpoints in `app/api/v1/ingestion.py`, enforce idempotency, channel taxonomy normalization, and dead letter queue routing.
      
      **Template Pattern:**
      1. Generate idempotency key: `f"{event.tenant_id}:{event.event_id}"`
      2. Normalize channel via `app/ingestion/taxonomy.normalize_channel()` (google_search, facebook_ads, email_marketing, etc.)
      3. Route validation failures to `dead_events` table
      4. Ensure session-scoped events (ephemeral `session_id`, no cross-session tracking)
      5. Use `app/ingestion/service.py` IngestionService class as implementation pattern
      
      **Reference Implementation:** `app/ingestion/service.py` IngestionService.ingest_event() method from Architecture Guide B0.4
      
      **Example:**
      ```python
      from app.ingestion.service import IngestionService
      from app.ingestion.taxonomy import normalize_channel, ChannelNormalizationError
      from app.schemas.ingestion import IngestEventRequest, IngestEventResponse
      
      @router.post("/events", response_model=IngestEventResponse)
      async def ingest_event(
          event: IngestEventRequest,
          correlation_id: str = Header(..., alias="X-Correlation-ID"),
          tenant_id: str = Depends(get_current_tenant),
          db: AsyncSession = Depends(get_db),
      ) -> IngestEventResponse:
          service = IngestionService(db)
          return await service.ingest_event(event, correlation_id)
      ```
      
      **Required Patterns:**
      - Idempotency key format: `{tenant_id}:{event_id}` (prevents duplicate ingestion)
      - Channel normalization: Use taxonomy mapping (google_search, facebook_ads, etc.), NOT hardcoded strings
      - Dead letter queue: Route invalid events to `dead_events` table with error context
      - Session-scoped: Generate ephemeral `session_id` for each event (30-minute timeout)
      - No PII: Strip all PII from event payloads before persistence

  - name: "Creating a new database model"
    description: |
      When creating new SQLAlchemy models in `app/models/`, enforce tenant isolation, Row-Level Security, and Skeldir's unified data system patterns.
      
      **Template Pattern:**
      1. Include `tenant_id UUID NOT NULL` column with FK to `tenants(id)`
      2. Create PostgreSQL RLS policy: `tenant_isolation_policy` using `current_setting('app.current_tenant_id')`
      3. Add composite indexes on `(tenant_id, timestamp DESC)` for time-series queries
      4. Store revenue as `INTEGER` (cents), NOT `DECIMAL` or `FLOAT`
      5. Use `JSONB` columns for raw payloads: `raw_payload JSONB NOT NULL`
      6. Implement processing status state machine: `pending`, `processed`, `failed`, `dead`
      
      **Reference Models:** `AttributionEvent`, `AttributionAllocation`, `RevenueLedger`, `DeadEvent` from Architecture Guide B0.3
      
      **Example:**
      ```python
      from sqlalchemy import Column, UUID, Integer, String, JSONB, TIMESTAMP, ForeignKey
      from sqlalchemy.dialects.postgresql import UUID as PGUUID
      from app.core.database import Base
      
      class YourModel(Base):
          __tablename__ = "your_table"
          
          id = Column(PGUUID(as_uuid=True), primary_key=True, default=gen_random_uuid)
          tenant_id = Column(PGUUID(as_uuid=True), ForeignKey("tenants(id)", ondelete="CASCADE"), nullable=False)
          
          # Revenue in cents (INTEGER)
          revenue_cents = Column(Integer, nullable=False)
          
          # Raw payload (JSONB)
          raw_payload = Column(JSONB, nullable=False)
          
          # Processing status state machine
          processing_status = Column(String(20), default="pending")  # pending, processed, failed, dead
          
          # Timestamps
          created_at = Column(TIMESTAMP(timezone=True), default=func.now())
          updated_at = Column(TIMESTAMP(timezone=True), onupdate=func.now())
          
          __table_args__ = (
              Index("idx_your_table_tenant_timestamp", "tenant_id", "created_at", postgresql_ops={"created_at": "DESC"}),
          )
      ```
      
      **Required Migration Pattern:**
      ```python
      # In Alembic migration file
      op.execute("ALTER TABLE your_table ENABLE ROW LEVEL SECURITY")
      op.execute("""
          CREATE POLICY tenant_isolation_policy ON your_table
              USING (tenant_id = current_setting('app.current_tenant_id')::UUID)
      """)
      ```
      
      **Required Patterns:**
      - All tenant-scoped tables MUST have RLS policies enabled
      - Revenue columns MUST be INTEGER (cents), never DECIMAL or FLOAT
      - Composite indexes on `(tenant_id, timestamp)` for time-series queries
      - Foreign key constraints with `ON DELETE CASCADE` for tenant cleanup
      - JSONB for semi-structured data (raw payloads, metadata)

  - name: "Creating a new background task"
    description: |
      When creating Celery background tasks in `app/tasks/`, enforce PostgreSQL broker usage and task observability patterns.
      
      **Template Pattern:**
      1. Configure Celery broker: `CELERY_BROKER_URL=db+postgresql://...` (NOT Redis or RabbitMQ)
      2. Use task naming convention: `app.tasks.{module}.{task_name}`
      3. Wrap async implementations with `@shared_task` and `asyncio.run()`
      4. Propagate correlation IDs from task context to logs
      5. For materialized view refresh: Use `REFRESH MATERIALIZED VIEW CONCURRENTLY`
      
      **Reference Tasks:** `app/tasks/attribution/`, `app/tasks/maintenance/` from Architecture Guide B0.5
      
      **Example:**
      ```python
      from celery import shared_task
      from app.core.celery_app import celery_app
      import asyncio
      import logging
      
      logger = logging.getLogger(__name__)
      
      @shared_task(
          bind=True,
          name="app.tasks.your_module.your_task",
          max_retries=3
      )
      def your_task(self):
          """
          Task description with correlation ID propagation.
          """
          try:
              asyncio.run(_your_task_async())
          except Exception as exc:
              logger.error(f"Task failed: {exc}", exc_info=True)
              raise self.retry(exc=exc)
      
      async def _your_task_async():
          # Async implementation with correlation ID in logs
          correlation_id = get_correlation_id_from_context()
          logger.info("Task started", extra={"correlation_id": correlation_id})
          # Task logic
      ```
      
      **Required Configuration:**
      ```python
      # app/core/celery_app.py
      celery_app = Celery(
          "skeldir",
          broker="db+postgresql://user:pass@localhost/dbname",  # PostgreSQL, NOT Redis
          backend="db+postgresql://user:pass@localhost/dbname",
      )
      ```
      
      **Required Patterns:**
      - Celery broker MUST use PostgreSQL (`db+postgresql://`), NEVER Redis (`redis://`) or RabbitMQ (`amqp://`)
      - Task names MUST follow convention: `app.tasks.{module}.{task_name}`
      - All tasks MUST propagate correlation IDs for distributed tracing
      - Materialized view refresh MUST use `CONCURRENTLY` to avoid locking
      - Task retries MUST include exponential backoff

  - name: "Implementing a statistical attribution model"
    description: |
      When implementing attribution models in `app/attribution/models/`, differentiate between deterministic and Bayesian models, enforce allocation completeness, and include model metadata.
      
      **Template Pattern:**
      1. **Deterministic Models** (`linear`, `last_touch`):
         - Require 100% allocation completeness: `sum(allocations) == conversion_value_cents`
         - No convergence diagnostics required
         - Examples: `DeterministicAttributionEngine` with linear or last-touch allocation
      
      2. **Bayesian Models** (`bayesian_mmm`):
         - Use `pymc-marketing` framework for Media Mix Modeling
         - Require convergence diagnostics (see Rule 6)
         - Store model version and type in allocations
      
      3. **Attribution Window:** 30-day lookback for touchpoint events
      
      **Reference Implementation:** `DeterministicAttributionEngine` and Bayesian MMM models from Architecture Guide B1.2-B1.3
      
      **Example (Deterministic):**
      ```python
      class DeterministicAttributionEngine:
          def __init__(self, attribution_window_days: int = 30):
              self.attribution_window_days = attribution_window_days
          
          async def allocate_conversion(
              self,
              conversion: AttributionEvent,
              model_type: str,  # "linear" or "last_touch"
              db: AsyncSession
          ) -> List[AttributionAllocation]:
              # Fetch touchpoint events within attribution window
              touchpoints = await self._get_touchpoints(conversion, db)
              
              # Allocate revenue based on model type
              allocations = self._allocate(model_type, touchpoints, conversion)
              
              # Validate 100% allocation completeness
              total_allocated = sum(a.allocated_revenue_cents for a in allocations)
              assert total_allocated == conversion.conversion_value_cents, \
                  f"Allocation incomplete: {total_allocated} != {conversion.conversion_value_cents}"
              
              return allocations
      ```
      
      **Example (Bayesian):**
      ```python
      import pymc_marketing as pm
      
      class BayesianMMM:
          async def allocate_conversion(
              self,
              conversion: AttributionEvent,
              db: AsyncSession
          ) -> List[AttributionAllocation]:
              # Build PyMC model
              with pm.Model() as model:
                  # Model specification
                  trace = pm.sample()
              
              # Extract allocations with convergence diagnostics
              allocations = self._extract_allocations(trace, conversion)
              
              # Store model metadata
              for allocation in allocations:
                  allocation.model_type = "bayesian_mmm"
                  allocation.model_version = "1.0.0"
              
              return allocations
      ```
      
      **Required Patterns:**
      - Deterministic models MUST achieve 100% allocation completeness
      - Bayesian models MUST use `pymc-marketing` framework
      - All allocations MUST include `model_type` and `model_version`
      - Attribution window MUST be 30 days (configurable but default)
      - Models are core defensible assets: document methodology and assumptions

  - name: "Statistical QA and convergence diagnostics"
    description: |
      When implementing Bayesian attribution models, enforce automated convergence diagnostics and auditable QA protocols.
      
      **Template Pattern:**
      1. Run convergence diagnostics using ArviZ after PyMC sampling
      2. Required checks: `R-hat < 1.01`, `ESS > 400`, `divergences == 0`
      3. Store diagnostics in `attribution_allocations` table: `convergence_r_hat`, `effective_sample_size`
      4. Include diagnostics in API responses for auditable QA
      5. Fail CI/CD builds if convergence diagnostics fail
      
      **Reference Protocol:** QA protocol B2.5 from Architecture Guide
      
      **Example:**
      ```python
      import arviz as az
      
      def validate_convergence(trace) -> dict:
          """
          Validates Bayesian model convergence diagnostics.
          
          Returns:
              dict with diagnostics: r_hat, ess, divergences
          
          Raises:
              ConvergenceError: If diagnostics fail thresholds
          """
          summary = az.summary(trace)
          
          r_hat_max = summary["r_hat"].max()
          ess_min = summary["ess_bulk"].min()
          divergences = trace["diverging"].sum()
          
          diagnostics = {
              "r_hat_max": float(r_hat_max),
              "ess_min": int(ess_min),
              "divergences": int(divergences),
          }
          
          # Validate thresholds
          if r_hat_max >= 1.01:
              raise ConvergenceError(f"R-hat {r_hat_max} >= 1.01")
          if ess_min < 400:
              raise ConvergenceError(f"ESS {ess_min} < 400")
          if divergences > 0:
              raise ConvergenceError(f"Divergences {divergences} > 0")
          
          return diagnostics
      
      # Store in allocation
      allocation.convergence_r_hat = diagnostics["r_hat_max"]
      allocation.effective_sample_size = diagnostics["ess_min"]
      ```
      
      **Required Patterns:**
      - All Bayesian models MUST pass convergence diagnostics before use
      - Diagnostics MUST be stored in database for auditability
      - API responses MUST include diagnostics for transparency
      - CI/CD MUST fail builds if diagnostics fail
      - Thresholds: R-hat < 1.01, ESS > 400, divergences == 0 (non-negotiable)

  - name: "Handling incoming webhook data"
    description: |
      When creating webhook handlers in `app/webhooks/`, enforce PII stripping, HMAC signature validation, and privacy-first data handling.
      
      **Template Pattern:**
      1. Create webhook-specific handlers: `app/webhooks/shopify.py`, `app/webhooks/stripe.py`, etc.
      2. Validate HMAC signatures for all webhook sources (Shopify, WooCommerce, Stripe, PayPal)
      3. Strip PII from webhook payloads before persistence (use PII stripping service)
      4. Generate ephemeral `session_id` for webhook-originated events (no user identity resolution)
      5. Use webhook-provided transaction IDs for idempotency keys
      6. Extract or generate correlation IDs from webhook headers
      
      **Reference Implementation:** Webhook handlers from Architecture Guide B0.6
      
      **Example:**
      ```python
      from fastapi import APIRouter, Header, HTTPException, Request
      import hmac
      import hashlib
      from app.ingestion.service import IngestionService
      from app.webhooks.pii_stripper import PIIStripper
      
      router = APIRouter(prefix="/webhooks", tags=["Webhooks"])
      
      @router.post("/shopify/orders/create")
      async def shopify_webhook(
          request: Request,
          x_shopify_hmac_sha256: str = Header(..., alias="X-Shopify-Hmac-Sha256"),
      ):
          # Validate HMAC signature
          body = await request.body()
          expected_signature = hmac.new(
              settings.SHOPIFY_WEBHOOK_SECRET.encode(),
              body,
              hashlib.sha256
          ).hexdigest()
          
          if not hmac.compare_digest(expected_signature, x_shopify_hmac_sha256):
              raise HTTPException(status_code=401, detail="Invalid signature")
          
          # Parse and strip PII
          payload = await request.json()
          stripped_payload = PIIStripper.strip(payload)
          
          # Generate ephemeral session_id (no user identity resolution)
          session_id = generate_ephemeral_session_id()
          
          # Create ingestion event with idempotency from transaction ID
          event = IngestEventRequest(
              tenant_id=get_tenant_from_webhook(payload),
              event_id=payload["order"]["id"],
              session_id=session_id,
              event_type="purchase",
              channel="shopify",
              conversion_value_cents=int(float(payload["total_price"]) * 100),
              timestamp=parse_shopify_timestamp(payload["created_at"]),
          )
          
          # Ingest via IngestionService
          service = IngestionService(db)
          return await service.ingest_event(event, correlation_id=generate_correlation_id())
      ```
      
      **Required Patterns:**
      - ALL webhooks MUST validate HMAC signatures before processing
      - ALL webhook payloads MUST pass through PII stripping before persistence
      - Session IDs MUST be ephemeral (no cross-session tracking)
      - Idempotency keys MUST use webhook transaction IDs
      - Correlation IDs MUST be extracted from headers or generated

  - name: "Creating OpenAPI contracts"
    description: |
      When creating API contracts in `api-contracts/openapi/v1/`, enforce contract-first workflow, semantic versioning, and breaking change detection.
      
      **Template Pattern:**
      1. Define OpenAPI 3.1 spec BEFORE implementation (contract-first)
      2. Use semantic versioning: `major.minor.patch` (major=breaking, minor=additive, patch=docs)
      3. Structure: Separate files per domain (`auth.yaml`, `attribution.yaml`, `webhooks/shopify.yaml`)
      4. Required fields: `X-Correlation-ID` header in all endpoints, `tenant_id` in all responses
      5. Generate Pydantic models using `datamodel-codegen` from contracts to `app/schemas/`
      
      **Reference Structure:** `api-contracts/openapi/v1/` from Architecture Guide B0.1
      
      **Example Contract:**
      ```yaml
      # api-contracts/openapi/v1/attribution.yaml
      openapi: 3.1.0
      info:
        title: Skeldir Attribution API
        version: 1.0.0
      
      paths:
        /api/attribution/revenue/realtime:
          get:
            operationId: getRealtimeRevenue
            parameters:
              - name: X-Correlation-ID
                in: header
                required: true
                schema:
                  type: string
                  format: uuid
            responses:
              '200':
                description: Success
                content:
                  application/json:
                    schema:
                      $ref: '#/components/schemas/RealtimeRevenueResponse'
      
      components:
        schemas:
          RealtimeRevenueResponse:
            type: object
            required:
              - total_revenue
              - tenant_id
            properties:
              total_revenue:
                type: number
              tenant_id:
                type: string
                format: uuid
      ```
      
      **Model Generation Script:**
      ```bash
      # scripts/generate-models.sh
      datamodel-codegen \
        --input api-contracts/openapi/v1/attribution.yaml \
        --output app/schemas/attribution.py \
        --target-python-version 3.11 \
        --use-annotated \
        --use-standard-collections
      ```
      
      **Required Patterns:**
      - Contracts MUST be defined BEFORE implementation (contract-first)
      - All endpoints MUST include `X-Correlation-ID` header parameter
      - All responses MUST include `tenant_id` field
      - Breaking changes MUST trigger major version bump
      - Use `oasdiff breaking` in CI/CD to detect breaking changes

  - name: "Generating Frontend SDK"
    description: |
      When generating the frontend SDK from OpenAPI contracts, enforce contract-first workflow and semantic versioning alignment.
      
      **Template Pattern:**
      1. Use `openapi-generator-cli` with Skeldir-specific configuration
      2. Generate from `api-contracts/openapi/v1/` directory
      3. Output to `frontend/src/generated/api/`
      4. Align SDK version with API semantic versioning
      
      **Reference Script:** `scripts/generate-sdk.sh` from Architecture Guide B0.1
      
      **Example Script:**
      ```bash
      #!/bin/bash
      # scripts/generate-sdk.sh
      # Generates TypeScript SDK from OpenAPI contracts
      
      npx @openapitools/openapi-generator-cli generate \
        -i api-contracts/openapi/v1/attribution.yaml \
        -g typescript-axios \
        -o frontend/src/generated/api/attribution \
        --additional-properties=supportsES6=true,withInterfaces=true,typescriptThreePlus=true
      
      npx @openapitools/openapi-generator-cli generate \
        -i api-contracts/openapi/v1/auth.yaml \
        -g typescript-axios \
        -o frontend/src/generated/api/auth \
        --additional-properties=supportsES6=true,withInterfaces=true,typescriptThreePlus=true
      ```
      
      **Required Patterns:**
      - SDK MUST be generated from OpenAPI contracts (contract-first)
      - SDK version MUST align with API semantic versioning
      - Output directory MUST be `frontend/src/generated/api/`
      - Regenerate SDK on every contract change (automated in CI/CD)

  - name: "Code quality and governance standards"
    description: |
      Enforce code quality, observability, and documentation standards across all Skeldir backend code.
      
      **Code Complexity:**
      - Cyclomatic complexity < 10 per function
      - Use static analysis: Ruff for linting, mypy for type checking
      - Pre-commit hooks: Ruff, mypy, black formatting
      
      **Test Coverage:**
      - Business logic: ≥80% coverage
      - Statistical models: ≥95% coverage (critical for defensible assets)
      - All public functions MUST have docstrings
      - Attribution endpoints MUST include statistical validation notes in docstrings
      
      **Observability:**
      - OpenTelemetry spans for all API endpoints
      - Structured logging: JSON format with `tenant_id`, `correlation_id`, `event_type` fields
      - Correlation ID propagation: All async operations must propagate correlation IDs
      
      **Documentation:**
      - All public functions MUST include docstrings
      - Attribution endpoints MUST document statistical validation requirements
      - Database models MUST document RLS policy behavior
      - Background tasks MUST document retry behavior and error handling
      
      **Example (Structured Logging):**
      ```python
      import logging
      
      logger = logging.getLogger(__name__)
      
      logger.info(
          "Event ingested",
          extra={
              "tenant_id": str(tenant_id),
              "correlation_id": correlation_id,
              "event_type": event.event_type,
              "channel": event.channel,
          }
      )
      ```
      
      **Example (OpenTelemetry):**
      ```python
      from opentelemetry import trace
      
      tracer = trace.get_tracer(__name__)
      
      @router.get("/endpoint")
      async def endpoint(correlation_id: str = Header(...)):
          with tracer.start_as_current_span("your_endpoint") as span:
              span.set_attribute("correlation_id", correlation_id)
              span.set_attribute("tenant_id", tenant_id)
              # Implementation
      ```
      
      **Required Patterns:**
      - All API endpoints MUST have OpenTelemetry spans
      - All logs MUST be structured JSON with correlation IDs
      - Test coverage MUST meet thresholds (80% business logic, 95% models)
      - All public functions MUST have docstrings with statistical notes for attribution code

---
prohibitions: |
  # Architectural Anti-Patterns (PE Deal-Killers)
  
  The following patterns are explicitly FORBIDDEN as they violate PE acquisition readiness mandates:
  
  ## Data Architecture Violations
  
  - ❌ **Redis/Kafka Usage:** Violates PostgreSQL-only mandate, creates disparate data systems
    - Use PostgreSQL for caching (materialized views) and event processing (tables + workers)
    - Use HTTP ETag caching (30-60s TTL) instead of Redis
  
  - ❌ **Revenue as DECIMAL/FLOAT:** Must use INTEGER (cents) for precision and performance
    - Database: `revenue_cents INTEGER NOT NULL`
    - API: Convert to float (dollars) in responses: `revenue_cents / 100.0`
  
  - ❌ **Missing tenant_id:** All tenant-scoped tables MUST have `tenant_id UUID NOT NULL`
  
  - ❌ **Missing RLS Policies:** All tenant-scoped tables MUST have Row-Level Security enabled
    - Policy pattern: `tenant_id = current_setting('app.current_tenant_id')::UUID`
  
  ## Privacy Violations
  
  - ❌ **Cross-Session User Journey Reconstruction:** Violates privacy-first mandate
    - Session IDs MUST be ephemeral (30-minute timeout)
    - NO cross-session tracking or user identity resolution
  
  - ❌ **Identity Stitching or Probabilistic Matching:** PII violation
    - NO user identity resolution algorithms
    - NO cross-device tracking logic
  
  - ❌ **PII Storage:** Names, emails, addresses MUST be stripped before persistence
    - All webhook data MUST pass through PII stripping service
  
  ## Code Quality Violations
  
  - ❌ **Missing Idempotency Keys:** Ingestion endpoints MUST use idempotency keys
    - Format: `f"{tenant_id}:{event_id}"`
    - Prevents duplicate event processing
  
  - ❌ **Hardcoded Channel Names:** MUST use channel taxonomy normalization
    - Use `app/ingestion/taxonomy.normalize_channel()` function
    - Standard taxonomy: google_search, facebook_ads, email_marketing, etc.
  
  - ❌ **Direct Database Queries:** MUST use SQLAlchemy models, not raw SQL
    - Violates abstraction and makes testing difficult
  
  - ❌ **Missing Convergence Diagnostics:** Bayesian models MUST include diagnostics
    - R-hat < 1.01, ESS > 400, divergences == 0
    - Store in `attribution_allocations` table
  
  - ❌ **Missing Correlation IDs:** All API endpoints MUST require `X-Correlation-ID` header
    - UUID format for distributed tracing
  
  ## Process Violations
  
  - ❌ **Implementation Before Contract:** MUST follow contract-first workflow
    - Define OpenAPI spec BEFORE implementation
    - Generate Pydantic models from contracts
  
  - ❌ **Missing Test Coverage:** Business logic ≥80%, statistical models ≥95%
    - CI/CD MUST fail if coverage drops below thresholds

---
quality_gates: |
  # Quality Gates and Validation
  
  ## Pre-Commit Hooks
  
  - **Ruff:** Linting and code quality checks
  - **mypy:** Static type checking
  - **black:** Code formatting (enforced)
  
  ## Code Review Requirements
  
  All pull requests MUST include:
  
  - **Contract Validation:** `oasdiff breaking` must pass (no breaking changes without major version bump)
  - **RLS Policy Verification:** All new tenant-scoped tables must have RLS policies
  - **Test Coverage Report:** Coverage must meet thresholds (80% business logic, 95% models)
  - **Convergence Diagnostics:** Bayesian models must pass diagnostics in tests
  
  ## CI/CD Validation
  
  - **API Contract Backward Compatibility:** `oasdiff breaking` must pass before merge
  - **Test Coverage Enforcement:** Fail CI if coverage drops below thresholds
  - **Statistical Model Validation:** Bayesian models must pass convergence diagnostics in tests
  - **Database Migration Validation:** All migrations must include RLS policy updates
  - **Type Checking:** mypy must pass with no errors
  
  ## Deployment Gates
  
  - **Breaking Changes:** Major version bump required, migration guide published 30 days before deployment
  - **Convergence Diagnostics:** All Bayesian models must pass diagnostics in production
  - **RLS Policies:** All tenant-scoped tables must have RLS enabled in production

